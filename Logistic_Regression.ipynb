{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "K0UZrpdn163k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/ What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        "- Logistic Regression is a statistical method used for binary classification problems, where the outcome variable is categorical with two possible values (e.g., 0 or 1, yes or no, true or false). It's a type of generalized linear model that uses a logistic function to predict the probability of the outcome.\n",
        "\n",
        "How does it differ from Linear Regression?\n",
        "\n",
        "Outcome Variable: Linear Regression is used to predict a continuous outcome variable, while Logistic Regression is used to predict a categorical outcome variable with two possible values.\n",
        "Model: Linear Regression uses a linear equation to model the relationship between the independent and dependent variables, while Logistic Regression uses a logistic function to model the relationship.\n",
        "Prediction: Linear Regression predicts the value of the outcome variable, while Logistic Regression predicts the probability of the outcome belonging to a particular category."
      ],
      "metadata": {
        "id": "5fWmosNB2E3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2/  What is the mathematical equation of Logistic Regression\n",
        "\n",
        "- p = 1 / (1 + exp(-(β0 + β1x1 + β2x2 + ... + βnxn)))"
      ],
      "metadata": {
        "id": "eGPYg7MU2XKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/  Why do we use the Sigmoid function in Logistic Regression.\n",
        "\n",
        "- Probability Output: The Sigmoid function outputs a value between 0 and 1, which is ideal for representing probabilities. In Logistic Regression, we want to predict the probability of an outcome belonging to a particular class (e.g., the probability of a customer churning or not). The Sigmoid function ensures that our predictions are always within the valid range of probabilities.\n",
        "\n",
        "S-Shaped Curve: The Sigmoid function has a characteristic \"S\" shape. This shape is useful for modeling the relationship between the independent variables and the probability of the outcome. As the value of the independent variable increases, the probability of the outcome also increases, but at a decreasing rate. This reflects the fact that probabilities cannot exceed 1.\n",
        "\n",
        "Differentiability: The Sigmoid function is differentiable, which is important for model training. Logistic Regression uses gradient descent to find the optimal values for the model's coefficients. Gradient descent requires the function to be differentiable so that it can calculate the gradient (slope) of the function and update the coefficients accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "kRgkQNvV2jI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/What is the cost function of Logistic Regression.\n",
        "\n",
        "- The cost function in Logistic Regression is called the Log Loss or Cross-Entropy Loss. It measures the difference between the predicted probabilities and the actual outcomes. The goal of Logistic Regression is to find the model parameters (coefficients) that minimize this cost function."
      ],
      "metadata": {
        "id": "u6kx8d4u25s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5/ What is Regularization in Logistic Regression? Why is it needed.\n",
        "\n",
        "- Okay, let's discuss Regularization in Logistic Regression:\n",
        "\n",
        "What is Regularization?\n",
        "\n",
        "Regularization is a technique used in machine learning to prevent overfitting. Overfitting occurs when a model learns the training data too well and performs poorly on unseen data. Regularization adds a penalty term to the cost function, discouraging the model from learning overly complex patterns that might only exist in the training data.\n",
        "\n",
        "Why is it needed in Logistic Regression?\n",
        "\n",
        "Prevent Overfitting: As mentioned earlier, regularization helps prevent overfitting by adding a penalty to the cost function for large coefficient values. This encourages the model to find a simpler solution that generalizes better to unseen data.\n",
        "Improve Generalization: By preventing overfitting, regularization improves the model's ability to generalize to new, unseen data, leading to better performance in real-world scenarios.\n",
        "Handle Multicollinearity: Regularization can help handle multicollinearity, which is the presence of highly correlated independent variables in the data. By shrinking the coefficients towards zero, regularization reduces the impact of multicollinearity on the model's performance."
      ],
      "metadata": {
        "id": "0shOdRzQ26Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6/Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "- Ridge Regression:\n",
        "\n",
        "Penalty: Adds a penalty proportional to the sum of squared coefficients (L2 penalty).\n",
        "Effect: Shrinks the coefficients towards zero, but doesn't make them exactly zero. This means all features are retained in the model, but their impact is reduced.\n",
        "Use Cases: Useful when there are many correlated features or when you want to keep all features in the model but reduce their influence.\n",
        "\n",
        "2. Lasso Regression:\n",
        "\n",
        "Penalty: Adds a penalty proportional to the sum of absolute values of the coefficients (L1 penalty).\n",
        "Effect: Can shrink some coefficients to exactly zero, effectively performing feature selection. This leads to a sparser model with fewer features.\n",
        "Use Cases: Useful when there are many irrelevant features or when you want to automatically select the most important features.\n",
        "\n",
        "3. Elastic Net Regression:\n",
        "\n",
        "Penalty: Combines both L1 and L2 penalties, using a mixing parameter (α) to control the balance between them.\n",
        "Effect: Can shrink some coefficients to zero (like Lasso) while also shrinking the remaining coefficients (like Ridge).\n",
        "Use Cases: Offers a compromise between Ridge and Lasso, providing benefits of both. Useful when there are many correlated features and some irrelevant features. It can also be more stable than Lasso in certain situations.\n"
      ],
      "metadata": {
        "id": "9ttAM2Tf260q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7/  When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "- Elastic Net is generally preferred over Lasso or Ridge in the following situations:\n",
        "\n",
        "When you have a dataset with many correlated features and some irrelevant features.\n",
        "\n",
        "Elastic Net combines the strengths of both Lasso and Ridge, allowing it to handle both multicollinearity (correlated features) and feature selection (irrelevant features).\n",
        "Lasso might struggle with multicollinearity and select only one feature from a group of correlated features, potentially missing out on important information.\n",
        "Ridge might not perform well in feature selection, as it shrinks all coefficients towards zero but doesn't eliminate any.\n",
        "When the number of features is greater than the number of observations (p > n).\n",
        "\n",
        "In high-dimensional datasets, Lasso might become unstable and select a random subset of features.\n",
        "Elastic Net is more stable in such scenarios and can provide better generalization performance."
      ],
      "metadata": {
        "id": "fzBSiFRr28Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8/What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "\n",
        "- Regularization Parameter (λ):\n",
        "\n",
        "The regularization parameter (λ), also known as the regularization strength or penalty parameter, controls the amount of regularization applied to the Logistic Regression model.\n",
        "It is a hyperparameter that needs to be tuned to find the optimal value for a given dataset.\n",
        "\n",
        "Impact of λ:\n",
        "\n",
        "Coefficient Shrinkage: Increasing λ increases the penalty for large coefficient values, leading to greater shrinkage of the coefficients towards zero. This helps prevent overfitting by discouraging the model from learning overly complex patterns in the training data.\n"
      ],
      "metadata": {
        "id": "300qzuiw28ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9/ What are the key assumptions of Logistic Regression.\n",
        "\n",
        "- Linearity of the Logit: The relationship between the independent variables and the logit (log-odds) of the outcome should be linear. This means that the log-odds of the outcome should change linearly with changes in the independent variables.\n",
        "\n",
        "Independence of Observations: The observations in the dataset should be independent of each other. This means that the outcome for one observation should not be influenced by the outcomes of other observations.\n",
        "\n",
        "No Multicollinearity: There should be little or no multicollinearity among the independent variables. Multicollinearity occurs when two or more independent variables are highly correlated with each other. It can make the model unstable and difficult to interpret.\n",
        "\n",
        "Large Sample Size: Logistic Regression generally requires a relatively large sample size to produce reliable estimates. The rule of thumb is to have at least 10 observations per independent variable in the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IxBsNCIX29CX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10/What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "- Here are some alternatives to Logistic Regression for classification tasks:\n",
        "\n",
        "Support Vector Machines (SVMs): SVMs are powerful models that can handle both linear and non-linear data. They work by finding the best hyperplane that separates the data into different classes.\n",
        "Decision Trees: Decision trees are tree-like structures that use a series of if-else conditions to classify data. They are easy to interpret and can handle both categorical and numerical data.\n",
        "Random Forests: Random forests are an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting. They are known for their high performance and robustness.\n",
        "Naive Bayes: Naive Bayes is a probabilistic classifier that uses Bayes' theorem to predict the class of a data point. It is simple to implement and can be effective for text classification and spam filtering.\n",
        "K-Nearest Neighbors (KNN): KNN is a non-parametric method that classifies data points based on the classes of their nearest neighbors. It is easy to understand and can be used for both classification and regression.\n"
      ],
      "metadata": {
        "id": "kc4h_GE929b8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11/ What are Classification Evaluation Metrics?\n",
        "\n",
        "- Classification evaluation metrics are used to assess the performance of a classification model. Here are some common metrics:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances.\n",
        "Precision: The proportion of true positive predictions among all positive predictions.\n",
        "Recall (Sensitivity): The proportion of true positive predictions among all actual positive instances.\n",
        "F1-Score: The harmonic mean of precision and recall, providing a balanced measure of performance.\n",
        "ROC AUC: The area under the receiver operating characteristic curve, measuring the model's ability to distinguish between classes.\n",
        "Log Loss: The negative average of the log of the predicted probabilities for the true class, indicating the model's confidence in its predictions."
      ],
      "metadata": {
        "id": "kDjNgjRS2-H1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12/ How does class imbalance affect Logistic Regression?\n",
        "\n",
        "- Class imbalance occurs when one class has significantly more instances than the other. This can affect Logistic Regression in the following ways:\n",
        "\n",
        "Bias towards Majority Class: The model may become biased towards the majority class and have poor performance on the minority class.\n",
        "Inaccurate Evaluation Metrics: Accuracy can be misleading when there is class imbalance, as a model can achieve high accuracy by simply predicting the majority class for all instances."
      ],
      "metadata": {
        "id": "uUwfYEbf2-jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13/ What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        " - Hyperparameter tuning is the process of finding the optimal values for the hyperparameters of a Logistic Regression model. Hyperparameters are settings that are not learned from the data but are set before training. Here are some common hyperparameters for Logistic Regression:\n",
        "\n",
        "Regularization Strength (λ or C): Controls the amount of regularization applied to the model.\n",
        "Penalty: Specifies the type of regularization (L1 or L2).\n",
        "Solver: Determines the algorithm used to optimize the model's parameters."
      ],
      "metadata": {
        "id": "F3gH2MNs5bPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14/ What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "- Logistic Regression in scikit-learn offers several solvers for optimizing the model's parameters. Here are some common solvers:\n",
        "\n",
        "'newton-cg', 'lbfgs', 'sag', 'saga': These solvers are suitable for large datasets and can handle L2 regularization.\n",
        "'liblinear': This solver is efficient for small datasets and supports both L1 and L2 regularization.\n",
        "The choice of solver depends on the size of the dataset, the type of regularization, and the desired performance. For large datasets with L2 regularization, 'sag' or 'saga' are recommended. For small datasets or L1 regularization, 'liblinear' is a good choice."
      ],
      "metadata": {
        "id": "KoXfy3b75btw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15/ How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "- Logistic Regression can be extended for multiclass classification using two main approaches:\n",
        "\n",
        "One-vs-Rest (OvR): This approach trains multiple binary classifiers, one for each class, to distinguish that class from all other classes.\n",
        "Multinomial (Softmax Regression): This approach directly models the probabilities of all classes using a softmax function."
      ],
      "metadata": {
        "id": "F3X068kQ5cOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16/ What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "-\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and easy to interpret.\n",
        "Efficient to train and predict.\n",
        "Handles both numerical and categorical data.\n",
        "Provides probability estimates for predictions.\n",
        "Can be regularized to prevent overfitting.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Assumes linearity of the logit.\n",
        "Can be sensitive to outliers.\n",
        "May not perform well with complex relationships between features and target.\n",
        "Requires a relatively large sample size."
      ],
      "metadata": {
        "id": "t6qEXt2R5cpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17/ What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "- Softmax Regression is a generalization of Logistic Regression for multiclass classification. While Logistic Regression is used for binary classification, Softmax Regression can handle multiple classes. Softmax Regression uses a softmax function to predict the probabilities of all classes, while Logistic Regression uses a sigmoid function to predict the probability of a single class."
      ],
      "metadata": {
        "id": "Yk_ux8955dFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18/ How do we interpret coefficients in Logistic Regression.\n",
        "\n",
        "- In Logistic Regression, the coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, holding all other variables constant. To make this more interpretable, we often exponentiate the coefficients to obtain odds ratios.\n",
        "\n",
        "Interpreting the Odds Ratios\n",
        "\n",
        "Odds Ratio > 1: Indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome.\n",
        "Odds Ratio < 1: Indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome.\n",
        "Odds Ratio = 1: Indicates that there is no association between the predictor variable and the outcome.\n",
        "Example:\n",
        "\n",
        "Let's say we have a Logistic Regression model predicting the likelihood of a customer churning (outcome: 1 = churn, 0 = no churn) based on their monthly charges (predictor). If the coefficient for monthly charges is 0.5, the odds ratio would be exp(0.5) ≈ 1.65.\n",
        "\n",
        "Interpretation: This means that for every one-unit increase in monthly charges, the odds of a customer churning increase by a factor of 1.65, or 65%, holding all other variables constant"
      ],
      "metadata": {
        "id": "M7s-OrDuWTsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "U8hjkkEm6ZTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiidwP4ZjEZ6",
        "outputId": "f2b160b0-de87-464a-e84d-a1df1b8b387a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY3fsYZNoFS4",
        "outputId": "7ff9a73a-b65e-4e5a-973b-937d8ffa5e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Index          User Id First Name Last Name  Gender  \\\n",
            "0        1  8717bbf45cCDbEe     Shelia   Mahoney    Male   \n",
            "1        2  3d5AD30A4cD38ed         Jo    Rivers  Female   \n",
            "2        3  810Ce0F276Badec     Sheryl    Lowery  Female   \n",
            "3        4  BF2a889C00f0cE1    Whitney    Hooper    Male   \n",
            "4        5  9afFEafAe1CBBB9    Lindsey      Rice  Female   \n",
            "..     ...              ...        ...       ...     ...   \n",
            "995    996  fedF4c7Fd9e7cFa       Kurt    Bryant  Female   \n",
            "996    997  ECddaFEDdEc4FAB      Donna     Barry  Female   \n",
            "997    998  2adde51d8B8979E      Cathy  Mckinney  Female   \n",
            "998    999  Fb2FE369D1E171A   Jermaine    Phelps    Male   \n",
            "999   1000  8b756f6231DDC6e        Lee      Tran  Female   \n",
            "\n",
            "                             Email                  Phone Date of birth  \\\n",
            "0              pwarner@example.org           857.139.8239    27-01-2014   \n",
            "1    fergusonkatherine@example.net                    NaN    26-07-1931   \n",
            "2              fhoward@example.org          (599)782-0605    25-11-2013   \n",
            "3            zjohnston@example.com                    NaN    17-11-2012   \n",
            "4                 elin@example.net     (390)417-1635x3010    15-04-1923   \n",
            "..                             ...                    ...           ...   \n",
            "995         lyonsdaisy@example.net           021.775.2933    05-01-1959   \n",
            "996        dariusbryan@example.com   001-149-710-7799x721    06-10-2001   \n",
            "997         georgechan@example.org  +1-750-774-4128x33265    13-05-1918   \n",
            "998            wanda04@example.net          (915)292-2254    31-08-1971   \n",
            "999        deannablack@example.org     079.752.5424x67259    24-01-1947   \n",
            "\n",
            "                           Job Title  Salary  \n",
            "0                  Probation officer   90000  \n",
            "1                             Dancer   80000  \n",
            "2                               Copy   50000  \n",
            "3           Counselling psychologist   65000  \n",
            "4                Biomedical engineer  100000  \n",
            "..                               ...     ...  \n",
            "995                Personnel officer   90000  \n",
            "996          Education administrator   50000  \n",
            "997  Commercial/residential surveyor   60000  \n",
            "998                 Ambulance person  100000  \n",
            "999       Nurse, learning disability   90000  \n",
            "\n",
            "[1000 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhjPin4kn9Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/ Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "l5puvf_2MglE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Iterate through columns and encode string columns\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "i8CGhKHnMgCK",
        "outputId": "a9cc08ae-22ee-49fc-da93-6840d3fbec15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.135\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2/ Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "TT_lTLm_L4ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "base_model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "\n",
        "ovo_classifier = OneVsOneClassifier(base_model)\n",
        "\n",
        "\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNpRYmpUMT6S",
        "outputId": "190ab83d-3e7c-4b23-9ac9-cc59ac074cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/ Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "eJ1w6KPKnKNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCfdvhV2uFut",
        "outputId": "ff80c2b5-70ca-425e-eebc-1de63a9d9794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.135\n",
            "Coefficients: [[-2.65305760e-05  2.24278098e-05 -1.10998339e-04 -5.07767453e-04\n",
            "   7.71733069e-02 -3.29872013e-04  3.74015892e-04 -1.87053814e-04\n",
            "   5.22043328e-04]\n",
            " [-1.83972024e-04 -4.85260855e-05  6.94952547e-04  7.27378429e-05\n",
            "   5.54380776e-02 -2.14819154e-04 -4.98750784e-04 -2.47918996e-04\n",
            "  -2.16362736e-03]\n",
            " [-4.12324571e-04 -5.73143216e-04 -1.24923758e-03 -1.24109754e-03\n",
            "   4.44565455e-01 -2.33626771e-05 -1.45972746e-05 -6.12785116e-04\n",
            "   6.35453057e-04]\n",
            " [-4.92444158e-04  8.91948577e-04 -3.62631688e-04 -2.98362020e-04\n",
            "  -1.54577256e-01 -8.48433093e-05  4.89802993e-05  6.76654122e-05\n",
            "  -1.60487515e-03]\n",
            " [ 2.31481143e-04 -6.93053829e-05 -5.50297182e-04 -3.06628432e-04\n",
            "  -8.79121817e-02 -1.92485777e-04 -2.25185006e-04  6.28784909e-06\n",
            "  -4.70284313e-05]\n",
            " [ 2.11937421e-05 -1.78563508e-04 -4.68962711e-04  2.07918362e-04\n",
            "  -1.73645095e-01 -1.81232314e-04  2.93236107e-05  7.03084197e-05\n",
            "  -3.64519104e-06]\n",
            " [-2.48769726e-04 -5.92992113e-04 -2.66590678e-04  6.59014019e-04\n",
            "  -3.71461674e-01  3.11553772e-04 -9.60965543e-04  8.79990595e-04\n",
            "   4.79348702e-04]\n",
            " [-8.89364394e-05  3.26271677e-05  3.50289281e-04 -1.55081020e-04\n",
            "   4.00522258e-02 -7.41352589e-05  2.91932590e-04 -8.94671945e-04\n",
            "   4.01414383e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/ Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "1ZIr5BNxHpNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwKegpdmHKhv",
        "outputId": "a1403478-cef7-4195-f831-17c6024e673a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.15\n",
            "Coefficients: [[ 5.98036139e-05  4.39878686e-05  7.90875827e-05 -2.90405645e-04\n",
            "   1.01052903e-04 -2.23399402e-04  3.95714176e-04 -9.17571017e-05\n",
            "   5.95598648e-04]\n",
            " [ 1.05398244e-04  1.03365588e-04  1.08162844e-03  4.55354517e-04\n",
            "   1.17219650e-04  2.24870940e-05 -2.21560774e-04  6.71145632e-06\n",
            "  -1.54032987e-03]\n",
            " [-8.05535337e-05 -3.70209279e-04 -5.32089112e-04 -5.78550987e-04\n",
            "   4.76727279e-04  2.36810388e-04  2.87385951e-04 -2.67312396e-04\n",
            "   1.08678380e-03]\n",
            " [-3.92958832e-04  8.13162724e-04 -2.74155620e-04 -2.51342042e-04\n",
            "  -9.61263988e-05 -7.20924005e-05  4.57731597e-05  6.06461005e-05\n",
            "  -1.42238633e-03]\n",
            " [ 3.22471895e-04 -1.49963654e-05 -2.93491498e-04 -1.21949502e-04\n",
            "  -5.02424632e-05 -9.40189036e-05 -1.21346847e-04  8.58997958e-05\n",
            "   1.18606911e-04]\n",
            " [ 1.42738760e-04 -1.05285574e-04 -2.20937869e-04  3.10755272e-04\n",
            "  -1.53070586e-04 -8.13961129e-05  9.77574054e-05  1.34550183e-04\n",
            "   1.56748686e-04]\n",
            " [-1.42621639e-04 -5.10767028e-04 -2.27105981e-04  5.54187342e-04\n",
            "  -3.61484923e-04  2.83630202e-04 -8.36236889e-04  7.67553004e-04\n",
            "   4.56547775e-04]\n",
            " [-1.47069002e-05  4.12412807e-05  4.32931962e-04 -3.35877266e-05\n",
            "   0.00000000e+00 -2.76765217e-05  3.08520136e-04 -7.39579949e-04\n",
            "   4.57757502e-04]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5/ Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "INElRHBDJUiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZxxV0j4JEBo",
        "outputId": "d89f67ad-2791-4b37-d134-a2261e7e52bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.135\n",
            "Coefficients: [[-2.65305760e-05  2.24278098e-05 -1.10998339e-04 -5.07767453e-04\n",
            "   7.71733069e-02 -3.29872013e-04  3.74015892e-04 -1.87053814e-04\n",
            "   5.22043328e-04]\n",
            " [-1.83972024e-04 -4.85260855e-05  6.94952547e-04  7.27378429e-05\n",
            "   5.54380776e-02 -2.14819154e-04 -4.98750784e-04 -2.47918996e-04\n",
            "  -2.16362736e-03]\n",
            " [-4.12324571e-04 -5.73143216e-04 -1.24923758e-03 -1.24109754e-03\n",
            "   4.44565455e-01 -2.33626771e-05 -1.45972746e-05 -6.12785116e-04\n",
            "   6.35453057e-04]\n",
            " [-4.92444158e-04  8.91948577e-04 -3.62631688e-04 -2.98362020e-04\n",
            "  -1.54577256e-01 -8.48433093e-05  4.89802993e-05  6.76654122e-05\n",
            "  -1.60487515e-03]\n",
            " [ 2.31481143e-04 -6.93053829e-05 -5.50297182e-04 -3.06628432e-04\n",
            "  -8.79121817e-02 -1.92485777e-04 -2.25185006e-04  6.28784909e-06\n",
            "  -4.70284313e-05]\n",
            " [ 2.11937421e-05 -1.78563508e-04 -4.68962711e-04  2.07918362e-04\n",
            "  -1.73645095e-01 -1.81232314e-04  2.93236107e-05  7.03084197e-05\n",
            "  -3.64519104e-06]\n",
            " [-2.48769726e-04 -5.92992113e-04 -2.66590678e-04  6.59014019e-04\n",
            "  -3.71461674e-01  3.11553772e-04 -9.60965543e-04  8.79990595e-04\n",
            "   4.79348702e-04]\n",
            " [-8.89364394e-05  3.26271677e-05  3.50289281e-04 -1.55081020e-04\n",
            "   4.00522258e-02 -7.41352589e-05  2.91932590e-04 -8.94671945e-04\n",
            "   4.01414383e-04]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6/ Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "Sk5lZMqIJskq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "\n",
        "model = LogisticRegression(solver='saga', max_iter=10000)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KABMcqiYJoU5",
        "outputId": "8ddfcad7-44bf-47ed-9ae1-f888c34bf886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l2'}\n",
            "Best Accuracy: 0.11624999999999999\n",
            "Test Accuracy: 0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7/ Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "JTQh9KzlJtOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "\n",
        "accuracy_scores = []\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "\n",
        "    model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "\n",
        "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "print(f\"Average Accuracy: {average_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5azVIvWJt0m",
        "outputId": "40b4ab68-d7a1-4e93-8991-8270aa71f1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.11099999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8/ Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "_mACLue-JuYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "best_accuracy = random_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-xD2V5HJu06",
        "outputId": "418ab00c-e5e4-48fc-81e3-6222221be7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 1438.44988828766}\n",
            "Best Accuracy: 0.1325\n",
            "Test Accuracy: 0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9/  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "549hJv2M__yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-fIKGeULLqLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c37be5d-ff7e-419f-dc4f-2c26ad4a31bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       50000       0.14      0.07      0.09        30\n",
            "       60000       0.14      0.14      0.14        36\n",
            "       65000       0.02      0.05      0.03        21\n",
            "       70000       0.12      0.20      0.15        20\n",
            "       80000       0.00      0.00      0.00        17\n",
            "       85000       0.43      0.11      0.18        27\n",
            "       90000       0.08      0.13      0.10        23\n",
            "      100000       0.16      0.15      0.16        26\n",
            "\n",
            "    accuracy                           0.11       200\n",
            "   macro avg       0.14      0.11      0.11       200\n",
            "weighted avg       0.15      0.11      0.11       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10/ Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "DHko7IPhAuUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "titanic_data = pd.read_csv('/content/drive/MyDrive/titanic .csv')\n",
        "\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "X = titanic_data[features]\n",
        "y = titanic_data[target]\n",
        "\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X[['Age', 'Fare']])\n",
        "X[['Age', 'Fare']] = X_imputed\n",
        "\n",
        "# Convert categorical features to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "X['Sex'] = label_encoder.fit_transform(X['Sex'])\n",
        "X['Embarked'] = label_encoder.fit_transform(X['Embarked'].astype(str))\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cq9YbebCIVh",
        "outputId": "187c2179-a33f-4a61-9753-10ce451bd165"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        50\n",
            "           1       1.00      1.00      1.00        34\n",
            "\n",
            "    accuracy                           1.00        84\n",
            "   macro avg       1.00      1.00      1.00        84\n",
            "weighted avg       1.00      1.00      1.00        84\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-45ba8329e69f>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[['Age', 'Fare']] = X_imputed\n",
            "<ipython-input-4-45ba8329e69f>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['Sex'] = label_encoder.fit_transform(X['Sex'])\n",
            "<ipython-input-4-45ba8329e69f>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['Embarked'] = label_encoder.fit_transform(X['Embarked'].astype(str))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11/ Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "ZbLau608FPei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Iterate through columns and encode string columns\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate without scaling\n",
        "model_no_scaling = LogisticRegression(solver='liblinear')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(solver='liblinear')\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaling}\")\n",
        "\n",
        "print(f\"Difference in accuracy: {accuracy_scaling - accuracy_no_scaling}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh2fWiz6CWRq",
        "outputId": "d48d430a-4f83-462d-8ad2-a3d60deb36e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.135\n",
            "Accuracy with scaling: 0.12\n",
            "Difference in accuracy: -0.015000000000000013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12/ Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "h3ngutHrFRx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(C=0.5, solver='liblinear')\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rc76FE5GT-v",
        "outputId": "536eea29-e356-42b6-9b18-d5ab5d2a602f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13/ M Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "ShnFbA7BGxKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "importance = model.coef_[0]\n",
        "\n",
        "feature_importance = dict(zip(X.columns, importance))\n",
        "\n",
        "sorted_features = sorted(feature_importance.items(), key=lambda item: abs(item[1]), reverse=True)\n",
        "print(\"Important Features:\")\n",
        "for feature, importance_score in sorted_features:\n",
        "    print(f\"{feature}: {importance_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUlods80Gt2x",
        "outputId": "03001b5c-0c38-420e-dd8f-ff4632397824"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features:\n",
            "Gender: 0.07717330685449007\n",
            "Job Title: 0.0005220433281155404\n",
            "Last Name: -0.0005077674531295302\n",
            "Phone: 0.0003740158916877357\n",
            "Email: -0.0003298720131429064\n",
            "Date of birth: -0.0001870538139749503\n",
            "First Name: -0.00011099833888477583\n",
            "Index: -2.653057603104962e-05\n",
            "User Id: 2.2427809771419085e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14/ Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "PuwKPc05GzVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Cohen's Kappa Score: {kappa_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R92gsj9sGz1K",
        "outputId": "25eb1af8-d3f8-4a1e-a37b-32152c38881e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: -0.006691882455629683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15/ Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "aFN-AMl2G0Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model_raw = LogisticRegression(solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy on raw data: {accuracy_raw}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scaled = LogisticRegression(solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled}\")\n",
        "\n",
        "# Compare results\n",
        "print(f\"Difference in accuracy: {accuracy_scaled - accuracy_raw}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejijGmZZG0xl",
        "outputId": "f184e5ec-6e29-4cec-801f-9efcc4af062b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.135\n",
            "Accuracy on standardized data: 0.12\n",
            "Difference in accuracy: -0.015000000000000013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16/ Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "5HLEyzHKIKuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Copy of People Data.csv')\n",
        "\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PONLOcBaID75",
        "outputId": "bf5aab68-df78-4a73-d2b8-523a3d262113"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.135\n"
          ]
        }
      ]
    }
  ]
}